# Spidey
A web crawler

Spidey is a simple web crawler that can be used to index webpages.
First (an) initial seed(s) are/is provided in the crawl_frontier text file. Then spidey will go through the URLs embedded in the seed(s), crawling and lurking through the world wide web, indexing every URL on its way.
Write now spidey only stores the URLs, the function store_page() is not intitialized. The job of this function is to archive the whole webpage, so that needs a bit of work.
To use simply clone the project then edit the paths and seeds according to your needs and hit run.
